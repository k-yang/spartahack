![alt text](https://challengepost-s3-challengepost.netdna-ssl.com/photos/production/software_photos/000/464/131/datas/gallery.jpg "Gestur logo")
# Gestur
An interaction platform for those with visual disability built @ Spartahack '17

## Inspiration:
We wanted to build something relating to computer vision and the theme of the hackathon was social impact. Eventually, our brainstorming session led us to develop a suite of tools to improve accessibility for the disabled. As we’ve had no direct experience with video processing at this scale, Interpreting American Sign Language seemed like a fair challenge. Through intense iteration we were able to narrow focus to helping to both streamline and redefine the quotidian interaction experience for American Sign Language speakers.

## What does it do?
It feeds photos from a webcam to a custom neural network which we trained on IBM Watson to determine the intent of the user. The intent will trigger a specific action from our backend and display useful data to the user.

For example, if a user is at a grocery store and signs “Where can I find apples?”, the webcam will pass this to our neural network, the neural network will extract the intent, and our application will show where where the apples in the grocery store are.

## Team members:
[Kevin](https://github.com/k-yang) [Simon](https://xmfan.github.io/)  [Victoria](https://devpost.com/VictoriaVandenberg) [Matthew](http:/matthewtse.xyz)

### Devpost
[link](https://devpost.com/software/gestur)
